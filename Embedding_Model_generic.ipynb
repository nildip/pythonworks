{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, Merge, Flatten, Dropout, Input, Reshape, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2, l1_l2\n",
    "from keras import utils as np_utils\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Production = True\n",
    "Rebalance = True\n",
    "Eliminate_Outlier = True\n",
    "cross_train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if Production == True:\n",
    "    # reading the training dataset\n",
    "    traindf = pd.read_csv('C:\\\\Users\\\\admin\\\\Downloads\\\\New folder\\\\train.csv',encoding='iso8859_2',low_memory=False)\n",
    "    # creating flag column for train\n",
    "    traindf['DataType'] = 'Train'\n",
    "    # reading the test dataset\n",
    "    testdf = pd.read_csv('C:\\\\Users\\\\admin\\\\Downloads\\\\New folder\\\\test.csv',encoding='iso8859_2',low_memory=False)\n",
    "    # savings list of ids of testdf for prediction in later stages\n",
    "    test_ids = testdf['UCIC_ID']\n",
    "    # creating the target variable column in test as 'N'\n",
    "    testdf['RESPONDERS'] = 'N'\n",
    "    # creating flag column for test\n",
    "    testdf['DataType'] = 'Test'\n",
    "    # combining train and test\n",
    "    df = (pd.concat([traindf.reset_index(drop = True),testdf.reset_index(drop = True)],axis=0))\n",
    "    del traindf\n",
    "    del testdf\n",
    "else:\n",
    "    df = pd.read_csv('C:\\\\Users\\\\admin\\\\Downloads\\\\New folder\\\\train.csv',encoding='iso8859_2',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subsetting variables by continous/categorical datatypes\n",
    "cont_vars = ['NO_OF_Accs','dependents','C_prev1','D_prev1','ATM_C_prev1','ATM_D_prev1','BRANCH_C_prev1',\n",
    "             'BRANCH_D_prev1','IB_C_prev1','IB_D_prev1','MB_C_prev1','MB_D_prev1','POS_C_prev1','POS_D_prev1','count_C_prev1',\n",
    "             'count_D_prev1','COUNT_ATM_C_prev1','COUNT_ATM_D_prev1','COUNT_BRANCH_C_prev1','COUNT_BRANCH_D_prev1',\n",
    "             'COUNT_IB_C_prev1','COUNT_IB_D_prev1','COUNT_MB_C_prev1','COUNT_MB_D_prev1','COUNT_POS_C_prev1','COUNT_POS_D_prev1',\n",
    "             'custinit_CR_amt_prev1','custinit_DR_amt_prev1','custinit_CR_cnt_prev1','custinit_DR_cnt_prev1','ATM_amt_prev1',\n",
    "             'ATM_CW_Amt_prev1','ATM_CW_Cnt_prev1','BRN_CW_Amt_prev1','BRN_CW_Cnt_prev1','BRN_CASH_Dep_Amt_prev1',\n",
    "             'BRN_CASH_Dep_Cnt_prev1','CNR_prev1','BAL_prev1','EOP_prev1','CR_AMB_Prev1','C_prev2','D_prev2','ATM_C_prev2',\n",
    "             'ATM_D_prev2','BRANCH_C_prev2','BRANCH_D_prev2','IB_C_prev2','IB_D_prev2','MB_C_prev2','MB_D_prev2','POS_C_prev2',\n",
    "             'POS_D_prev2','count_C_prev2','count_D_prev2','COUNT_ATM_C_prev2','COUNT_ATM_D_prev2','COUNT_BRANCH_C_prev2',\n",
    "             'COUNT_BRANCH_D_prev2','COUNT_IB_C_prev2','COUNT_IB_D_prev2','COUNT_MB_C_prev2','COUNT_MB_D_prev2',\n",
    "             'COUNT_POS_C_prev2','COUNT_POS_D_prev2','custinit_CR_amt_prev2','custinit_DR_amt_prev2','custinit_CR_cnt_prev2',\n",
    "             'custinit_DR_cnt_prev2','ATM_amt_prev2','ATM_CW_Amt_prev2','ATM_CW_Cnt_prev2','BRN_CW_Amt_prev2','BRN_CW_Cnt_prev2',\n",
    "             'BRN_CASH_Dep_Amt_prev2','BRN_CASH_Dep_Cnt_prev2','CNR_prev2','BAL_prev2','EOP_prev2','CR_AMB_Prev2','C_prev3',\n",
    "             'D_prev3','ATM_C_prev3','ATM_D_prev3','BRANCH_C_prev3','BRANCH_D_prev3','IB_C_prev3','IB_D_prev3','MB_C_prev3',\n",
    "             'MB_D_prev3','POS_C_prev3','POS_D_prev3','count_C_prev3','count_D_prev3','COUNT_ATM_C_prev3','COUNT_ATM_D_prev3',\n",
    "             'COUNT_BRANCH_C_prev3','COUNT_BRANCH_D_prev3','COUNT_IB_C_prev3','COUNT_IB_D_prev3','COUNT_MB_C_prev3',\n",
    "             'COUNT_MB_D_prev3','COUNT_POS_C_prev3','COUNT_POS_D_prev3','custinit_CR_amt_prev3','custinit_DR_amt_prev3',\n",
    "             'custinit_CR_cnt_prev3','custinit_DR_cnt_prev3','ATM_amt_prev3','ATM_CW_Amt_prev3','ATM_CW_Cnt_prev3',\n",
    "             'BRN_CW_Amt_prev3','BRN_CW_Cnt_prev3','BRN_CASH_Dep_Amt_prev3','BRN_CASH_Dep_Cnt_prev3','CNR_prev3','BAL_prev3',\n",
    "             'EOP_prev3','CR_AMB_Prev3','C_prev4','D_prev4','ATM_C_prev4','ATM_D_prev4','BRANCH_C_prev4','BRANCH_D_prev4',\n",
    "             'IB_C_prev4','IB_D_prev4','MB_C_prev4','MB_D_prev4','POS_C_prev4','POS_D_prev4','count_C_prev4','count_D_prev4',\n",
    "             'COUNT_ATM_C_prev4','COUNT_ATM_D_prev4','COUNT_BRANCH_C_prev4','COUNT_BRANCH_D_prev4','COUNT_IB_C_prev4',\n",
    "             'COUNT_IB_D_prev4','COUNT_MB_C_prev4','COUNT_MB_D_prev4','COUNT_POS_C_prev4','COUNT_POS_D_prev4',\n",
    "             'custinit_CR_amt_prev4','custinit_DR_amt_prev4','custinit_CR_cnt_prev4','custinit_DR_cnt_prev4','ATM_amt_prev4',\n",
    "             'ATM_CW_Amt_prev4','ATM_CW_Cnt_prev4','BRN_CW_Amt_prev4','BRN_CW_Cnt_prev4','BRN_CASH_Dep_Amt_prev4',\n",
    "             'BRN_CASH_Dep_Cnt_prev4','CNR_prev4','BAL_prev4','EOP_prev4','CR_AMB_Prev4','C_prev5','D_prev5','ATM_C_prev5',\n",
    "             'ATM_D_prev5','BRANCH_C_prev5','BRANCH_D_prev5','IB_C_prev5','IB_D_prev5','MB_C_prev5','MB_D_prev5','POS_C_prev5',\n",
    "             'POS_D_prev5','count_C_prev5','count_D_prev5','COUNT_ATM_C_prev5','COUNT_ATM_D_prev5','COUNT_BRANCH_C_prev5',\n",
    "             'COUNT_BRANCH_D_prev5','COUNT_IB_C_prev5','COUNT_IB_D_prev5','COUNT_MB_C_prev5','COUNT_MB_D_prev5',\n",
    "             'COUNT_POS_C_prev5','COUNT_POS_D_prev5','custinit_CR_amt_prev5','custinit_DR_amt_prev5','custinit_CR_cnt_prev5',\n",
    "             'custinit_DR_cnt_prev5','ATM_amt_prev5','ATM_CW_Amt_prev5','ATM_CW_Cnt_prev5','BRN_CW_Amt_prev5','BRN_CW_Cnt_prev5',\n",
    "             'BRN_CASH_Dep_Amt_prev5','BRN_CASH_Dep_Cnt_prev5','CNR_prev5','BAL_prev5','EOP_prev5','CR_AMB_Prev5','C_prev6',\n",
    "             'D_prev6','ATM_C_prev6','ATM_D_prev6','BRANCH_C_prev6','BRANCH_D_prev6','IB_C_prev6','IB_D_prev6','MB_C_prev6',\n",
    "             'MB_D_prev6','POS_C_prev6','POS_D_prev6','count_C_prev6','count_D_prev6','COUNT_ATM_C_prev6','COUNT_ATM_D_prev6',\n",
    "             'COUNT_BRANCH_C_prev6','COUNT_BRANCH_D_prev6','COUNT_IB_C_prev6','COUNT_IB_D_prev6','COUNT_MB_C_prev6',\n",
    "             'COUNT_MB_D_prev6','COUNT_POS_C_prev6','COUNT_POS_D_prev6','custinit_CR_amt_prev6','custinit_DR_amt_prev6',\n",
    "             'custinit_CR_cnt_prev6','custinit_DR_cnt_prev6','ATM_amt_prev6','ATM_CW_Amt_prev6','ATM_CW_Cnt_prev6',\n",
    "             'BRN_CW_Amt_prev6','BRN_CW_Cnt_prev6','BRN_CASH_Dep_Amt_prev6','BRN_CASH_Dep_Cnt_prev6','CNR_prev6','BAL_prev6',\n",
    "             'EOP_prev6','CR_AMB_Prev6','Billpay_Reg_ason_Prev1','FD_AMOUNT_BOOK_PrevQ1','FD_AMOUNT_BOOK_PrevQ2',\n",
    "             'NO_OF_FD_BOOK_PrevQ1','NO_OF_FD_BOOK_PrevQ2','NO_OF_RD_BOOK_PrevQ1','NO_OF_RD_BOOK_PrevQ2','RD_AMOUNT_BOOK_PrevQ1',\n",
    "             'RD_AMOUNT_BOOK_PrevQ2','Total_Invest_in_MF_PrevQ1','Total_Invest_in_MF_PrevQ2','count_No_of_MF_PrevQ1',\n",
    "             'count_No_of_MF_PrevQ2','Dmat_Investing_PrevQ1','Dmat_Investing_PrevQ2','Charges_PrevQ1','Charges_cnt_PrevQ1',\n",
    "             'NO_OF_COMPLAINTS','CASH_WD_AMT_Last6','CASH_WD_CNT_Last6','age','Recency_of_CR_TXN','Recency_of_DR_TXN',\n",
    "             'Recency_of_IB_TXN','Recency_of_ATM_TXN','Recency_of_BRANCH_TXN','Recency_of_POS_TXN','Recency_of_MB_TXN',\n",
    "             'Recency_of_Activity','I_AQB_PrevQ1','I_AQB_PrevQ2','I_CR_AQB_PrevQ1','I_CR_AQB_PrevQ2','I_CNR_PrevQ1',\n",
    "             'I_CNR_PrevQ2','I_NRV_PrevQ1','I_NRV_PrevQ2','CR_AMB_Drop_Build_1','CR_AMB_Drop_Build_2','CR_AMB_Drop_Build_3',\n",
    "             'CR_AMB_Drop_Build_4','CR_AMB_Drop_Build_5','Req_Logged_PrevQ1','Req_Resolved_PrevQ1','Query_Logged_PrevQ1',\n",
    "             'Query_Resolved_PrevQ1','Complaint_Logged_PrevQ1','Complaint_Resolved_PrevQ1','NO_OF_CHEQUE_BOUNCE_V1',\n",
    "             'Percent_Change_in_Credits','Percent_Change_in_FT_Bank','Percent_Change_in_FT_outside','Percent_Change_in_Self_Txn',\n",
    "             'Percent_Change_in_Big_Expenses']\n",
    "dummy_vars = ['HNW_CATEGORY','FINAL_WORTH_prev1','EMAIL_UNSUBSCRIBE','ENGAGEMENT_TAG_prev1','FRX_PrevQ1',\n",
    "              'EFT_SELF_TRANSFER_PrevQ1','Billpay_Active_PrevQ1','AGRI_PREM_CLOSED_PREVQ1','AL_CNC_PREM_CLOSED_PREVQ1',\n",
    "              'AL_PREM_CLOSED_PREVQ1','BL_PREM_CLOSED_PREVQ1','CC_PREM_CLOSED_PREVQ1','CE_PREM_CLOSED_PREVQ1',\n",
    "              'CV_PREM_CLOSED_PREVQ1','EDU_PREM_CLOSED_PREVQ1','OTHER_LOANS_PREM_CLOSED_PREVQ1','PL_PREM_CLOSED_PREVQ1',\n",
    "              'RD_PREM_CLOSED_PREVQ1','FD_PREM_CLOSED_PREVQ1','TL_PREM_CLOSED_PREVQ1','TWL_PREM_CLOSED_PREVQ1',\n",
    "              'AGRI_Closed_PrevQ1','AL_CNC_Closed_PrevQ1','AL_Closed_PrevQ1','BL_Closed_PrevQ1','CC_CLOSED_PREVQ1',\n",
    "              'CE_Closed_PrevQ1','CV_Closed_PrevQ1','EDU_Closed_PrevQ1','GL_Closed_PrevQ1','OTHER_LOANS_Closed_PrevQ1',\n",
    "              'PL_Closed_PrevQ1','RD_CLOSED_PREVQ1','FD_CLOSED_PREVQ1','TL_Closed_PrevQ1','TWL_Closed_PrevQ1',\n",
    "              'DEMAT_CLOSED_PREV1YR','SEC_ACC_CLOSED_PREV1YR','AGRI_TAG_LIVE','AL_CNC_TAG_LIVE','AL_TAG_LIVE','BL_TAG_LIVE',\n",
    "              'CC_TAG_LIVE','CE_TAG_LIVE','CV_TAG_LIVE','DEMAT_TAG_LIVE','EDU_TAG_LIVE','GL_TAG_LIVE','HL_TAG_LIVE',\n",
    "              'SEC_ACC_TAG_LIVE','INS_TAG_LIVE','LAS_TAG_LIVE','MF_TAG_LIVE','OTHER_LOANS_TAG_LIVE','PL_TAG_LIVE','RD_TAG_LIVE',\n",
    "              'FD_TAG_LIVE','TL_TAG_LIVE','TWL_TAG_LIVE','lap_tag_live','Billpay_Active_PrevQ1_N','Billpay_Reg_ason_Prev1_N',\n",
    "              'Charges_cnt_PrevQ1_N','FRX_PrevQ1_N','RBI_Class_Audit','gender_bin']\n",
    "# creating a ordered dict for embedding variables -> (varaible name : 'K'-d vector)\n",
    "emb_vars_dict = OrderedDict([('OCCUP_ALL_NEW',5),('city',5),('zip',5),('brn_code',5)])\n",
    "emb_vars = [evar for (evar, esize) in emb_vars_dict.items()]\n",
    "# defining target/outcome variable\n",
    "target_var = ['Responders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some columns have '>' printed instead of '>9'. we set those values to 10\n",
    "for i in cont_vars:\n",
    "    if df[i].dtype == 'O':\n",
    "        df[i] = np.where(df[i] == '>','10',df[i])\n",
    "    else:\n",
    "        df[i] = df[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in cont_vars:\n",
    "    df[i] = pd.to_numeric(df[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in dummy_vars:\\n    if df[i].dtype == 'O':\\n        df[i] = df[i].fillna('N')\\n    else:\\n        df[i] = df[i].fillna(0)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replacing NA values by 'N' for character type columns and 0 for numeric type columns\n",
    "df[cont_vars] = df[cont_vars].fillna(0)\n",
    "'''\n",
    "for i in dummy_vars:\n",
    "    if df[i].dtype == 'O':\n",
    "        df[i] = df[i].fillna('N')\n",
    "    else:\n",
    "        df[i] = df[i].fillna(0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "outlier_elim() missing 1 required positional argument: 'upper_limit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-1c6da82ec205>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcont_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mupper_limit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutlier_elim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2353\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2354\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2355\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2357\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\src\\inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer (pandas\\_libs\\lib.c:66645)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: outlier_elim() missing 1 required positional argument: 'upper_limit'"
     ]
    }
   ],
   "source": [
    "if Eliminate_Outlier == True:\n",
    "    def outlier_elim(x,limit):\n",
    "        if x > limit: \n",
    "            return limit\n",
    "        else: \n",
    "            return x\n",
    "    # setting a upper cutoff at Mean + (2 * std.dev)\n",
    "    for i in cont_vars:\n",
    "        upper_limit = df[i].mean() + 2*df[i].std()\n",
    "        df.loc[:,i] = df.loc[:,i].apply(outlier_elim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for applying label encoding to variables\n",
    "def labelencoder(input_df,var_list):\n",
    "    le = LabelEncoder()\n",
    "    for var in var_list:\n",
    "        try:\n",
    "            input_df[var] = le.fit_transform(input_df[var])\n",
    "        except:\n",
    "            input_df[var] = input_df[var].astype(str)\n",
    "            input_df[var] = le.fit_transform(input_df[var])\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if Production == True:\n",
    "    # subsetting df to contain only relevant variables\n",
    "    df = df[cont_vars+dummy_vars+emb_vars+target_var+['DataType']]\n",
    "else:\n",
    "    # subsetting df to contain only relevant variables\n",
    "    df = df[cont_vars+dummy_vars+emb_vars+target_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# applying 'labelencoder' function for emb_vars\n",
    "df = labelencoder(df,emb_vars+target_var)\n",
    "# applying one hot encoding to categorical variables\n",
    "dummy_df = pd.get_dummies(df[dummy_vars],drop_first = True)\n",
    "#concatenating dummy_df to master_df\n",
    "df = (pd.concat([df.reset_index(drop = True),dummy_df.reset_index(drop = True)],axis = 1))\n",
    "#dropping 'non one hot encoded' dummy variable columns\n",
    "df = df.drop(dummy_vars, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scaling continous variables\n",
    "scaler = StandardScaler()\n",
    "df[cont_vars] = scaler.fit_transform(df.loc[:,cont_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if Production == True:\n",
    "    # resplitting train & test dfs\n",
    "    train_df = df[df.DataType == 'Train']\n",
    "    test_df = df[df.DataType == 'Test']\n",
    "    # dropping 'DataType' column\n",
    "    train_df = train_df.drop('DataType', 1)\n",
    "    test_df = test_df.drop('DataType', 1)\n",
    "    train_df = shuffle(train_df, random_state=666)\n",
    "    del df\n",
    "else:\n",
    "    # splitting train & test dfs\n",
    "    train_df,test_df = train_test_split(df,test_size = 0.3,random_state = 666)\n",
    "    del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if Rebalance == True:\n",
    "    # upsampling train_df\n",
    "    ros = RandomOverSampler(random_state = 666)\n",
    "    train_dftmp,y = ros.fit_sample(train_df,train_df[target_var]) \n",
    "    train_dftmp = pd.DataFrame(train_dftmp)\n",
    "    train_dftmp.columns = train_df.columns\n",
    "    train_df = train_dftmp\n",
    "    train_df = shuffle(train_df, random_state=666)\n",
    "    del train_dftmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dummy variables\n",
    "train_dummy_df = train_df.iloc[:,int(len(cont_vars))+int(len(emb_vars))+1:train_df.shape[1]]\n",
    "test_dummy_df = test_df.iloc[:,int(len(cont_vars))+int(len(emb_vars))+1:test_df.shape[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# continous variables\n",
    "train_cont_df = train_df[cont_vars]\n",
    "test_cont_df = test_df[cont_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embedded variables\n",
    "train_x_emb = train_df[emb_vars]\n",
    "test_x_emb = test_df[emb_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target variable\n",
    "y = pd.get_dummies(train_df[target_var],drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine continous and dummy variables to a single numpy array\n",
    "train_x_cont_dummy = (pd.concat([train_dummy_df.reset_index(drop = True),train_cont_df.reset_index(drop = True)],axis=1)).values\n",
    "test_x_cont_dummy = (pd.concat([test_dummy_df.reset_index(drop = True),test_cont_df.reset_index(drop = True)],axis=1)).values\n",
    "# creating the final train files for target variable\n",
    "train_y = train_df[target_var].values\n",
    "# creating the final train and test files for embedded variable\n",
    "train_x_emb = train_x_emb.values\n",
    "test_x_emb = test_x_emb.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for creating hidden embeddeding layers for variables\n",
    "def EmbeddingCreator (emb_var_dict,input_df):\n",
    "    id_list = ['encoder_' + var for (var, size) in emb_var_dict.items()] #adding 'encoder_' to each variable name\n",
    "    embeddingvars = {k: Sequential() for k in id_list} #creating a sequential layer for each variable\n",
    "    for embvar,nn_obj in embeddingvars.items():\n",
    "        colname = embvar.replace('encoder_', '') #removing 'encoder_' from each variable name\n",
    "        nn_obj.add(Embedding(len(input_df[colname].unique()),emb_var_dict[colname],input_length = 1,\n",
    "                             embeddings_regularizer = l2(1e-2))) #adding embedding layer for each variable\n",
    "        nn_obj.add(Flatten()) #compressing layer to 1-D plane\n",
    "    embedding_vars = [embeddingvars[embvar] for embvar in id_list] #extracting NN model objects\n",
    "    return embedding_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# applying 'EmbeddingCreator' function for creating hidden embeddeding layers for variables\n",
    "df = (pd.concat([train_df.reset_index(drop = True),test_df.reset_index(drop = True)],axis=0))\n",
    "emb_layers = EmbeddingCreator(emb_vars_dict,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining fully connected layers\n",
    "# input layer for continous and binary variables\n",
    "dense_x = Sequential()\n",
    "dense_x.add(Dense(250, input_dim=train_x_cont_dummy.shape[1]))\n",
    "all_layers = emb_layers+[dense_x]\n",
    "# input layer for embedded variables\n",
    "model_emb = Sequential()\n",
    "model_emb.add(Merge(all_layers,mode='concat'))\n",
    "model_emb.add(Dense(units=180))\n",
    "model_emb.add(Dropout(0.30))\n",
    "model_emb.add(BatchNormalization())\n",
    "model_emb.add(Activation('relu'))\n",
    "model_emb.add(Dense(units=100))\n",
    "model_emb.add(Dropout(0.25))\n",
    "model_emb.add(BatchNormalization())\n",
    "model_emb.add(Activation('linear'))\n",
    "model_emb.add(Dense(units=50))\n",
    "model_emb.add(Dropout(0.15))\n",
    "model_emb.add(BatchNormalization())\n",
    "model_emb.add(Activation('relu'))\n",
    "model_emb.add(Dense(units=50))\n",
    "model_emb.add(Dropout(0.15))\n",
    "model_emb.add(BatchNormalization())\n",
    "model_emb.add(Activation('relu'))\n",
    "model_emb.add(Dense(units=1))\n",
    "model_emb.add(Activation('sigmoid'))\n",
    "model_emb.compile(optimizer='adagrad',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function to combine input features into a list\n",
    "def comb_emb_cont_dummy(emb_dict,x_emb,x_cont_dummy):\n",
    "    x_all = []\n",
    "    for i in range(len(emb_dict)):\n",
    "        x_all.append(x_emb[:,i])\n",
    "    x_all.append(x_cont_dummy)\n",
    "    return x_all\n",
    "\n",
    "train_x_all = comb_emb_cont_dummy(emb_vars_dict,train_x_emb,train_x_cont_dummy)\n",
    "test_x_all = comb_emb_cont_dummy(emb_vars_dict,test_x_emb,test_x_cont_dummy)\n",
    "\n",
    "# defining class weights\n",
    "class_weight = {0: 1.0,1: 1.0}\n",
    "\n",
    "# fitting the model\n",
    "model_emb.fit(train_x_all,train_y,epochs = 25,validation_split = 0.3,batch_size = 64,shuffle = True,class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if cross_train == True:\n",
    "    for i in range(1,20):\n",
    "        if i % 2 == 0:\n",
    "            class_weight = {0: 1.0,1: 2.0}\n",
    "        else:\n",
    "            class_weight = {0: 2.0,1: 1.0}\n",
    "        model_emb.fit(train_x_all,train_y,epochs = 1,validation_split = 0.3,batch_size = 64,shuffle = True,\n",
    "                      class_weight = class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# getting the predictions\n",
    "predicted_y_prob = model_emb.predict_proba(test_x_all) #probability scores\n",
    "if Production == False:\n",
    "    test_y = test_df[target_var]\n",
    "    # concatinating predicted scores and true labels\n",
    "    predict_df = (pd.concat([pd.DataFrame(predicted_y_prob).reset_index(drop=True),\n",
    "                             pd.DataFrame(test_y).reset_index(drop=True)], axis=1))\n",
    "    predict_df.columns = ['Predicted_Probability','True_Class']\n",
    "    predict_df.True_Class = predict_df.True_Class.astype(int)\n",
    "else:\n",
    "    predict_df = (pd.concat([pd.DataFrame(test_ids).reset_index(drop=True),\n",
    "                             pd.DataFrame(predicted_y_prob).reset_index(drop=True)], axis=1))\n",
    "    predict_df.columns = ['UCIC_ID','Responders']\n",
    "    predict_df.to_csv('C:\\\\Users\\\\admin\\\\Downloads\\\\New folder\\\\output.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if Production == False:\n",
    "    # True 0 - Probability distribution\n",
    "    predict_df_TrueLost = predict_df[predict_df['True_Class'] == 0]\n",
    "    sns.distplot(predict_df_TrueLost.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if Production == False:    \n",
    "    # True 1 - Probability distribution\n",
    "    predict_df_TrueWon = predict_df[predict_df['True_Class'] == 1]\n",
    "    sns.distplot(predict_df_TrueWon.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def liftcalculater(predict_df):\n",
    "    predict_df = predict_df.sort_values('Predicted_Probability',ascending = False)\n",
    "    predict_df = predict_df.reset_index(drop=True)\n",
    "    predict_df_top2decile = predict_df.iloc[:int(predict_df.shape[0]/5),:]\n",
    "    # finding % of responders in predict_df_top2decile\n",
    "    Total_Responders = sum(predict_df.True_Class)\n",
    "    FirstDecile_Responders = sum(predict_df_top2decile.True_Class)\n",
    "    Lift = FirstDecile_Responders/Total_Responders\n",
    "    return Lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if Production == False:\n",
    "    lift = liftcalculater(predict_df)\n",
    "    print(lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Code run completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
